{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 python-whois dnspython pygeoip reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import whois\n",
    "import dns.resolver\n",
    "import dns.reversename\n",
    "import pygeoip\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "import time\n",
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/mbcc2006/GeoLiteCity-data/raw/master/GeoLiteCity.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_web_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_content(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = [link['href'] for link in soup.find_all('a', href=True)]\n",
    "    text_content = soup.get_text()\n",
    "    return links, text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_info(domain):\n",
    "    retries = 3\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            domain_info = whois.whois(domain)\n",
    "            return domain_info\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            time.sleep(5)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dns_info(domain):\n",
    "    try:\n",
    "        result = dns.resolver.resolve(domain, 'A')\n",
    "        return [ip.to_text() for ip in result]\n",
    "    except Exception as e:\n",
    "        print(f\"DNS resolution error: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_mx_info(domain):\n",
    "    try:\n",
    "        result = dns.resolver.resolve(domain, 'MX')\n",
    "        return [mx.to_text() for mx in result]\n",
    "    except Exception as e:\n",
    "        print(f\"DNS MX resolution error: {e}\")\n",
    "        return []\n",
    "\n",
    "def reverse_dns_lookup(ip):\n",
    "    try:\n",
    "        addr = dns.reversename.from_address(ip)\n",
    "        result = dns.resolver.resolve(addr, 'PTR')\n",
    "        return [ptr.to_text() for ptr in result]\n",
    "    except Exception as e:\n",
    "        print(f\"Reverse DNS lookup error: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_ip_geolocation(ip):\n",
    "    try:\n",
    "        geo = pygeoip.GeoIP('GeoLiteCity.dat')\n",
    "        return geo.record_by_addr(ip)\n",
    "    except Exception as e:\n",
    "        print(f\"Geolocation error: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import Paragraph, Table, TableStyle, SimpleDocTemplate, Spacer, PageBreak\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "def create_pdf_report(url, content, links, text_content, domain_info, dns_info, mx_info, reverse_dns, geo_info):\n",
    "    pdf_file = 'report.pdf'\n",
    "    doc = SimpleDocTemplate(pdf_file, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    flowables = []\n",
    "\n",
    "    # Define custom styles\n",
    "    styleH = ParagraphStyle(\n",
    "        name='Heading1',\n",
    "        fontSize=14,\n",
    "        leading=16,\n",
    "        alignment=1,  # Center align\n",
    "        spaceAfter=12,\n",
    "        textColor=colors.black,\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "\n",
    "    styleB = styles['BodyText']\n",
    "    styleB.fontSize = 10\n",
    "    styleB.leading = 12\n",
    "\n",
    "    # Title\n",
    "    title = Paragraph(f\"URL Report for {url}\", styleH)\n",
    "    flowables.append(title)\n",
    "    flowables.append(Spacer(1, 12))\n",
    "\n",
    "    # Table of Contents\n",
    "    toc_title = Paragraph(\"Table of Contents\", styleH)\n",
    "    flowables.append(toc_title)\n",
    "    flowables.append(Spacer(1, 12))\n",
    "\n",
    "    toc = [\n",
    "        [\"Web Content\"],\n",
    "        [\"Extracted Links\"],\n",
    "        [\"Domain Information\"],\n",
    "        [\"DNS Information\"],\n",
    "        [\"MX Information\"],\n",
    "        [\"Reverse DNS Information\"],\n",
    "        [\"Geolocation Information\"]\n",
    "    ]\n",
    "\n",
    "    toc_table = Table(toc, colWidths=[6*inch])\n",
    "    toc_table.setStyle(TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "    ]))\n",
    "    flowables.append(toc_table)\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    def add_header(text):\n",
    "        header = Paragraph(text, styleH)\n",
    "        flowables.append(header)\n",
    "        flowables.append(Spacer(1, 6))\n",
    "\n",
    "    def add_paragraph(text):\n",
    "        para = Paragraph(text, styleB)\n",
    "        flowables.append(para)\n",
    "        flowables.append(Spacer(1, 12))\n",
    "\n",
    "    def add_table(data, col_widths):\n",
    "        table = Table(data, colWidths=col_widths)\n",
    "        table.setStyle(TableStyle([\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "        ]))\n",
    "        flowables.append(table)\n",
    "        flowables.append(Spacer(1, 12))\n",
    "\n",
    "    # Add Web Content\n",
    "    add_header(\"Web Content\")\n",
    "    add_paragraph(text_content[:3000])  # Add only first 3000 characters for simplicity\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    # Add Extracted Links\n",
    "    add_header(\"Extracted Links\")\n",
    "    if links:\n",
    "        links_table = [[Paragraph(link, styleB)] for link in links]\n",
    "        add_table(links_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No extracted links available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    # Add Domain Information\n",
    "    add_header(\"Domain Information\")\n",
    "    if domain_info:\n",
    "        domain_table = [[Paragraph(f\"{key}: {value}\", styleB)] for key, value in domain_info.items()]\n",
    "        add_table(domain_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"Failed to retrieve domain information.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    # Add DNS Information\n",
    "    add_header(\"DNS Information\")\n",
    "    if dns_info:\n",
    "        dns_table = [[Paragraph(dns, styleB)] for dns in dns_info]\n",
    "        add_table(dns_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No DNS information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    # Add MX Information\n",
    "    add_header(\"MX Information\")\n",
    "    if mx_info:\n",
    "        mx_table = [[Paragraph(mx, styleB)] for mx in mx_info]\n",
    "        add_table(mx_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No MX information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    # Add Reverse DNS Information\n",
    "    add_header(\"Reverse DNS Information\")\n",
    "    if reverse_dns:\n",
    "        reverse_dns_table = [[Paragraph(reverse_dns, styleB)] for reverse_dns in reverse_dns]\n",
    "        add_table(reverse_dns_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No reverse DNS information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    # Add Geolocation Information\n",
    "    add_header(\"Geolocation Information\")\n",
    "    if geo_info:\n",
    "        geo_table = [[Paragraph(f\"{key}: {value}\", styleB)] for key, value in geo_info.items()]\n",
    "        add_table(geo_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No geolocation information available.\")\n",
    "\n",
    "    doc.build(flowables)\n",
    "    return pdf_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url):\n",
    "    # Extract domain from URL\n",
    "    domain = urlparse(url).netloc\n",
    "\n",
    "    web_content = fetch_web_content(url)\n",
    "    links, text_content = parse_html_content(web_content)\n",
    "    domain_info = get_domain_info(domain)\n",
    "    dns_info = get_dns_info(domain)\n",
    "    mx_info = get_mx_info(domain)\n",
    "    reverse_dns = reverse_dns_lookup(dns_info[0]) if dns_info else []\n",
    "    geo_info = get_ip_geolocation(dns_info[0]) if dns_info else {}\n",
    "\n",
    "    pdf_file = create_pdf_report(url, web_content, links, text_content, domain_info, dns_info, mx_info, reverse_dns, geo_info)\n",
    "    return pdf_file\n",
    "\n",
    "# Example URL\n",
    "url = \"https://www.google.com\n",
    "4. To download the generated PDF:\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "files.download('report.pdf')\n",
    "\n",
    "4. To download the generated PDF:\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "files.download('report.pdf')\n",
    "are\"\n",
    "pdf_report = main(url)\n",
    "from google.colab import files\n",
    "files.download(pdf_report)\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import whois\n",
    "import dns.resolver\n",
    "import dns.reversename\n",
    "import pygeoip\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import Paragraph, Table, TableStyle, SimpleDocTemplate, Spacer, PageBreak\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def ensure_geolite_data():\n",
    "    geolite_path = 'GeoLiteCity.dat'\n",
    "    if not os.path.exists(geolite_path):\n",
    "        print(\"Downloading GeoLiteCity.dat...\")\n",
    "        url = \"https://github.com/mbcc2006/GeoLiteCity-data/raw/master/GeoLiteCity.dat\"\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, geolite_path)\n",
    "            print(\"Download complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading GeoLiteCity.dat: {e}\")\n",
    "            sys.exit(1)\n",
    "    return geolite_path\n",
    "\n",
    "def fetch_web_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def parse_html_content(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = [link['href'] for link in soup.find_all('a', href=True)]\n",
    "    text_content = soup.get_text()\n",
    "    return links, text_content\n",
    "\n",
    "def get_domain_info(domain):\n",
    "    retries = 3\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            domain_info = whois.whois(domain)\n",
    "            return domain_info\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            time.sleep(5)\n",
    "    return None\n",
    "\n",
    "def get_dns_info(domain):\n",
    "    try:\n",
    "        result = dns.resolver.resolve(domain, 'A')\n",
    "        return [ip.to_text() for ip in result]\n",
    "    except Exception as e:\n",
    "        print(f\"DNS resolution error: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_mx_info(domain):\n",
    "    try:\n",
    "        result = dns.resolver.resolve(domain, 'MX')\n",
    "        return [mx.to_text() for mx in result]\n",
    "    except Exception as e:\n",
    "        print(f\"DNS MX resolution error: {e}\")\n",
    "        return []\n",
    "\n",
    "def reverse_dns_lookup(ip):\n",
    "    try:\n",
    "        addr = dns.reversename.from_address(ip)\n",
    "        result = dns.resolver.resolve(addr, 'PTR')\n",
    "        return [ptr.to_text() for ptr in result]\n",
    "    except Exception as e:\n",
    "        print(f\"Reverse DNS lookup error: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_ip_geolocation(ip):\n",
    "    try:\n",
    "        geolite_path = ensure_geolite_data()\n",
    "        geo = pygeoip.GeoIP(geolite_path)\n",
    "        return geo.record_by_addr(ip)\n",
    "    except Exception as e:\n",
    "        print(f\"Geolocation error: {e}\")\n",
    "        return {}\n",
    "\n",
    "def create_pdf_report(url, content, links, text_content, domain_info, dns_info, mx_info, reverse_dns, geo_info):\n",
    "    pdf_file = 'report.pdf'\n",
    "    doc = SimpleDocTemplate(pdf_file, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    flowables = []\n",
    "\n",
    "    styleH = ParagraphStyle(\n",
    "        name='Heading1',\n",
    "        fontSize=14,\n",
    "        leading=16,\n",
    "        alignment=1,\n",
    "        spaceAfter=12,\n",
    "        textColor=colors.black,\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "\n",
    "    styleB = styles['BodyText']\n",
    "    styleB.fontSize = 10\n",
    "    styleB.leading = 12\n",
    "\n",
    "    title = Paragraph(f\"URL Report for {url}\", styleH)\n",
    "    flowables.append(title)\n",
    "    flowables.append(Spacer(1, 12))\n",
    "\n",
    "    toc_title = Paragraph(\"Table of Contents\", styleH)\n",
    "    flowables.append(toc_title)\n",
    "    flowables.append(Spacer(1, 12))\n",
    "\n",
    "    toc = [\n",
    "        [\"Web Content\"],\n",
    "        [\"Extracted Links\"],\n",
    "        [\"Domain Information\"],\n",
    "        [\"DNS Information\"],\n",
    "        [\"MX Information\"],\n",
    "        [\"Reverse DNS Information\"],\n",
    "        [\"Geolocation Information\"]\n",
    "    ]\n",
    "\n",
    "    toc_table = Table(toc, colWidths=[6*inch])\n",
    "    toc_table.setStyle(TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "    ]))\n",
    "    flowables.append(toc_table)\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    def add_header(text):\n",
    "        header = Paragraph(text, styleH)\n",
    "        flowables.append(header)\n",
    "        flowables.append(Spacer(1, 6))\n",
    "\n",
    "    def add_paragraph(text):\n",
    "        para = Paragraph(text, styleB)\n",
    "        flowables.append(para)\n",
    "        flowables.append(Spacer(1, 12))\n",
    "\n",
    "    def add_table(data, col_widths):\n",
    "        table = Table(data, colWidths=col_widths)\n",
    "        table.setStyle(TableStyle([\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "        ]))\n",
    "        flowables.append(table)\n",
    "        flowables.append(Spacer(1, 12))\n",
    "\n",
    "    add_header(\"Web Content\")\n",
    "    add_paragraph(text_content[:3000])\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"Extracted Links\")\n",
    "    if links:\n",
    "        links_table = [[Paragraph(link, styleB)] for link in links]\n",
    "        add_table(links_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No extracted links available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"Domain Information\")\n",
    "    if domain_info:\n",
    "        domain_table = [[Paragraph(f\"{key}: {value}\", styleB)] for key, value in domain_info.items()]\n",
    "        add_table(domain_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"Failed to retrieve domain information.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"DNS Information\")\n",
    "    if dns_info:\n",
    "        dns_table = [[Paragraph(dns, styleB)] for dns in dns_info]\n",
    "        add_table(dns_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No DNS information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"MX Information\")\n",
    "    if mx_info:\n",
    "        mx_table = [[Paragraph(mx, styleB)] for mx in mx_info]\n",
    "        add_table(mx_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No MX information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"Reverse DNS Information\")\n",
    "    if reverse_dns:\n",
    "        reverse_dns_table = [[Paragraph(reverse_dns, styleB)] for reverse_dns in reverse_dns]\n",
    "        add_table(reverse_dns_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No reverse DNS information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"Geolocation Information\")\n",
    "    if geo_info:\n",
    "        geo_table = [[Paragraph(f\"{key}: {value}\", styleB)] for key, value in geo_info.items()]\n",
    "        add_table(geo_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No geolocation information available.\")\n",
    "\n",
    "    doc.build(flowables)\n",
    "    return pdf_file\n",
    "\n",
    "def format_url(url):\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "def main():\n",
    "    ensure_geolite_data()\n",
    "\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python domain_analysis.py <url>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    url = format_url(sys.argv[1])\n",
    "    domain = urlparse(url).netloc\n",
    "\n",
    "    print(f\"Analyzing domain: {domain}\")\n",
    "\n",
    "    try:\n",
    "        web_content = fetch_web_content(url)\n",
    "        links, text_content = parse_html_content(web_content)\n",
    "        domain_info = get_domain_info(domain)\n",
    "        dns_info = get_dns_info(domain)\n",
    "        mx_info = get_mx_info(domain)\n",
    "        reverse_dns = reverse_dns_lookup(dns_info[0]) if dns_info else []\n",
    "        geo_info = get_ip_geolocation(dns_info[0]) if dns_info else {}\n",
    "\n",
    "        pdf_file = create_pdf_report(url, web_content, links, text_content, domain_info, dns_info, mx_info, reverse_dns, geo_info)\n",
    "        print(f\"PDF report generated: {pdf_file}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching web content: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import whois\n",
    "import dns.resolver\n",
    "import dns.reversename\n",
    "import pygeoip\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import Paragraph, Table, TableStyle, SimpleDocTemplate, Spacer, PageBreak\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def ensure_geolite_data():\n",
    "    geolite_path = 'GeoLiteCity.dat'\n",
    "    if not os.path.exists(geolite_path):\n",
    "        print(\"Downloading GeoLiteCity.dat...\")\n",
    "        url = \"https://github.com/mbcc2006/GeoLiteCity-data/raw/master/GeoLiteCity.dat\"\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, geolite_path)\n",
    "            print(\"Download complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading GeoLiteCity.dat: {e}\")\n",
    "            sys.exit(1)\n",
    "    return geolite_path\n",
    "\n",
    "def fetch_web_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def parse_html_content(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = [link['href'] for link in soup.find_all('a', href=True)]\n",
    "    text_content = soup.get_text()\n",
    "    return links, text_content\n",
    "\n",
    "def get_domain_info(domain):\n",
    "    retries = 3\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            domain_info = whois.whois(domain)\n",
    "            return domain_info\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying...\")\n",
    "            time.sleep(5)\n",
    "    return None\n",
    "\n",
    "def get_dns_info(domain):\n",
    "    try:\n",
    "        result = dns.resolver.resolve(domain, 'A')\n",
    "        return [ip.to_text() for ip in result]\n",
    "    except Exception as e:\n",
    "        print(f\"DNS resolution error: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_mx_info(domain):\n",
    "    try:\n",
    "        result = dns.resolver.resolve(domain, 'MX')\n",
    "        return [mx.to_text() for mx in result]\n",
    "    except Exception as e:\n",
    "        print(f\"DNS MX resolution error: {e}\")\n",
    "        return []\n",
    "\n",
    "def reverse_dns_lookup(ip):\n",
    "    try:\n",
    "        addr = dns.reversename.from_address(ip)\n",
    "        result = dns.resolver.resolve(addr, 'PTR')\n",
    "        return [ptr.to_text() for ptr in result]\n",
    "    except Exception as e:\n",
    "        print(f\"Reverse DNS lookup error: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_ip_geolocation(ip):\n",
    "    try:\n",
    "        geolite_path = ensure_geolite_data()\n",
    "        geo = pygeoip.GeoIP(geolite_path)\n",
    "        return geo.record_by_addr(ip)\n",
    "    except Exception as e:\n",
    "        print(f\"Geolocation error: {e}\")\n",
    "        return {}\n",
    "\n",
    "def create_pdf_report(url, content, links, text_content, domain_info, dns_info, mx_info, reverse_dns, geo_info):\n",
    "    pdf_file = 'report.pdf'\n",
    "    doc = SimpleDocTemplate(pdf_file, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    flowables = []\n",
    "\n",
    "    styleH = ParagraphStyle(\n",
    "        name='Heading1',\n",
    "        fontSize=14,\n",
    "        leading=16,\n",
    "        alignment=1,\n",
    "        spaceAfter=12,\n",
    "        textColor=colors.black,\n",
    "        fontName='Helvetica-Bold'\n",
    "    )\n",
    "\n",
    "    styleB = styles['BodyText']\n",
    "    styleB.fontSize = 10\n",
    "    styleB.leading = 12\n",
    "\n",
    "    title = Paragraph(f\"URL Report for {url}\", styleH)\n",
    "    flowables.append(title)\n",
    "    flowables.append(Spacer(1, 12))\n",
    "\n",
    "    toc_title = Paragraph(\"Table of Contents\", styleH)\n",
    "    flowables.append(toc_title)\n",
    "    flowables.append(Spacer(1, 12))\n",
    "\n",
    "    toc = [\n",
    "        [\"Web Content\"],\n",
    "        [\"Extracted Links\"],\n",
    "        [\"Domain Information\"],\n",
    "        [\"DNS Information\"],\n",
    "        [\"MX Information\"],\n",
    "        [\"Reverse DNS Information\"],\n",
    "        [\"Geolocation Information\"]\n",
    "    ]\n",
    "\n",
    "    toc_table = Table(toc, colWidths=[6*inch])\n",
    "    toc_table.setStyle(TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "    ]))\n",
    "    flowables.append(toc_table)\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    def add_header(text):\n",
    "        header = Paragraph(text, styleH)\n",
    "        flowables.append(header)\n",
    "        flowables.append(Spacer(1, 6))\n",
    "\n",
    "    def add_paragraph(text):\n",
    "        para = Paragraph(text, styleB)\n",
    "        flowables.append(para)\n",
    "        flowables.append(Spacer(1, 12))\n",
    "\n",
    "    def add_table(data, col_widths):\n",
    "        table = Table(data, colWidths=col_widths)\n",
    "        table.setStyle(TableStyle([\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "        ]))\n",
    "        flowables.append(table)\n",
    "        flowables.append(Spacer(1, 12))\n",
    "\n",
    "    add_header(\"Web Content\")\n",
    "    add_paragraph(text_content[:3000])\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"Extracted Links\")\n",
    "    if links:\n",
    "        links_table = [[Paragraph(link, styleB)] for link in links]\n",
    "        add_table(links_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No extracted links available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"Domain Information\")\n",
    "    if domain_info:\n",
    "        domain_table = [[Paragraph(f\"{key}: {value}\", styleB)] for key, value in domain_info.items()]\n",
    "        add_table(domain_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"Failed to retrieve domain information.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"DNS Information\")\n",
    "    if dns_info:\n",
    "        dns_table = [[Paragraph(dns, styleB)] for dns in dns_info]\n",
    "        add_table(dns_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No DNS information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"MX Information\")\n",
    "    if mx_info:\n",
    "        mx_table = [[Paragraph(mx, styleB)] for mx in mx_info]\n",
    "        add_table(mx_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No MX information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"Reverse DNS Information\")\n",
    "    if reverse_dns:\n",
    "        reverse_dns_table = [[Paragraph(reverse_dns, styleB)] for reverse_dns in reverse_dns]\n",
    "        add_table(reverse_dns_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No reverse DNS information available.\")\n",
    "    flowables.append(PageBreak())\n",
    "\n",
    "    add_header(\"Geolocation Information\")\n",
    "    if geo_info:\n",
    "        geo_table = [[Paragraph(f\"{key}: {value}\", styleB)] for key, value in geo_info.items()]\n",
    "        add_table(geo_table, [6*inch])\n",
    "    else:\n",
    "        add_paragraph(\"No geolocation information available.\")\n",
    "\n",
    "    doc.build(flowables)\n",
    "    return pdf_file\n",
    "\n",
    "def format_url(url):\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "def main():\n",
    "    ensure_geolite_data()\n",
    "\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python domain_analysis.py <url>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    url = format_url(sys.argv[1])\n",
    "    domain = urlparse(url).netloc\n",
    "\n",
    "    print(f\"Analyzing domain: {domain}\")\n",
    "\n",
    "    try:\n",
    "        web_content = fetch_web_content(url)\n",
    "        links, text_content = parse_html_content(web_content)\n",
    "        domain_info = get_domain_info(domain)\n",
    "        dns_info = get_dns_info(domain)\n",
    "        mx_info = get_mx_info(domain)\n",
    "        reverse_dns = reverse_dns_lookup(dns_info[0]) if dns_info else []\n",
    "        geo_info = get_ip_geolocation(dns_info[0]) if dns_info else {}\n",
    "\n",
    "        pdf_file = create_pdf_report(url, web_content, links, text_content, domain_info, dns_info, mx_info, reverse_dns, geo_info)\n",
    "        print(f\"PDF report generated: {pdf_file}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching web content: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
